
# ğŸ¦ Twitter Data Pipeline

Este projeto implementa um pipeline de dados moderno utilizando **Apache Airflow**, **AWS S3** e **Snowflake** para orquestrar a ingestÃ£o, transformaÃ§Ã£o e carregamento de tweets. Ideal para fins de portfÃ³lio, aprendizado ou projetos de engenharia de dados em produÃ§Ã£o.

## ğŸ“Œ VisÃ£o Geral

O pipeline realiza as seguintes etapas:

1. **ExtraÃ§Ã£o**: carrega tweets de um arquivo `.json`.
2. **TransformaÃ§Ã£o**: normaliza e limpa os dados, convertendo para `.parquet`.
3. **Armazenamento**: envia o `.parquet` para um bucket AWS S3.
4. **Carga final**: carrega os dados do `.parquet` do S3 para uma tabela Snowflake.

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.11**
- **Apache Airflow**
- **Docker + Docker Compose**
- **AWS S3**
- **Snowflake**
- **Pandas**
- **Boto3 (AWS SDK)**
- **Snowflake Connector for Python**

## ğŸ—‚ï¸ Estrutura do Projeto

```
twitter-data-papeline-main/
â”œâ”€â”€ dags/                      # DAG principal do Airflow
â”‚   â””â”€â”€ twitter_dag.py
â”œâ”€â”€ scripts/                   # Scripts de integraÃ§Ã£o com S3 e Snowflake
â”‚   â”œâ”€â”€ extract_and_transform.py
â”‚   â””â”€â”€ load_to_snowflake.py
â”‚   â””â”€â”€ upload_to_s3.py 
â”œâ”€â”€ data/                      # Dados de exemplo
â”‚   â””â”€â”€ tweets_sample.json
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config_example.yaml    # ConfiguraÃ§Ãµes sensÃ­veis (ex: S3, Snowflake)
â”œâ”€â”€ docker-compose.yaml        # Ambientes Airflow
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente (Airflow, AWS, Snowflake)
```

## ğŸ” Fluxo do Pipeline

A DAG do Airflow executa as seguintes tarefas:

1. **Extrair dados do JSON**
2. **Transformar para .parquet**
3. **Fazer upload para AWS S3**
4. **Carregar no Snowflake**

## ğŸ“¸ Diagrama do Pipeline

<p align="center">
  <img src="pipeline_diagram.png" width="600px">
</p>

## â–¶ï¸ Como Executar Localmente

### PrÃ©-requisitos

- Docker + Docker Compose
- Conta na AWS com bucket S3 criado
- Conta na Snowflake com:
  - Warehouse: `COMPUTE_WH`
  - Database: `TWITTER_DATA_PIPELINE`
  - Schema: `RAW_DATA`
  - Tabela: `tweets` criada previamente

### 1. Clone o projeto

```bash
git clone https://github.com/dgcastrooo/twitter-data-papeline.git
cd twitter-data-papeline
```

### 2. Configure suas variÃ¡veis

Crie o arquivo `.env` com as seguintes variÃ¡veis (exemplo):

```env
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_BUCKET_NAME=your_bucket_name
AWS_REGION=sa-east-1

SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_DATABASE=TWITTER_DATA_PIPELINE
SNOWFLAKE_SCHEMA=RAW_DATA
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_TABLE=tweets
```

### 3. Suba os containers

```bash
docker-compose up -d
```

### 4. Acesse o Airflow

Abra no navegador:

```
http://localhost:8080
```

Login padrÃ£o:

- **UsuÃ¡rio:** `airflow`
- **Senha:** `airflow`

## âœ… Status das Tarefas

- [x] ExtraÃ§Ã£o de dados do JSON
- [x] TransformaÃ§Ã£o para Parquet
- [x] Upload para AWS S3
- [x] Carga para Snowflake
- [x] OrquestraÃ§Ã£o via Airflow

## ğŸ“„ LicenÃ§a

DistribuÃ­do sob a licenÃ§a MIT. Veja `LICENSE` para mais informaÃ§Ãµes.

## ğŸ™‹â€â™‚ï¸ Autor

**Diogo JosÃ© Castro de Carvalho Pereira**  
[LinkedIn](https://www.linkedin.com/in/dgcastrooo/) â€¢ [GitHub](https://github.com/dgcastrooo)
