
# üê¶ Twitter Data Pipeline

Este projeto implementa um pipeline de dados moderno utilizando **Apache Airflow**, **AWS S3** e **Snowflake** para orquestrar a ingest√£o, transforma√ß√£o e carregamento de tweets. Ideal para fins de portf√≥lio, aprendizado ou projetos de engenharia de dados em produ√ß√£o.

## üìå Vis√£o Geral

O pipeline realiza as seguintes etapas:

1. **Extra√ß√£o**: carrega tweets de um arquivo `.json`.
2. **Transforma√ß√£o**: normaliza e limpa os dados, convertendo para `.parquet`.
3. **Armazenamento**: envia o `.parquet` para um bucket AWS S3.
4. **Carga final**: carrega os dados do `.parquet` do S3 para uma tabela Snowflake.

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.11**
- **Apache Airflow**
- **Docker + Docker Compose**
- **AWS S3**
- **Snowflake**
- **Pandas**
- **Boto3 (AWS SDK)**
- **Snowflake Connector for Python**

## üóÇÔ∏è Estrutura do Projeto

```
twitter-data-papeline-main/
‚îú‚îÄ‚îÄ dags/                      # DAG principal do Airflow
‚îÇ   ‚îî‚îÄ‚îÄ twitter_dag.py
‚îú‚îÄ‚îÄ scripts/                   # Scripts de integra√ß√£o com S3 e Snowflake
‚îÇ   ‚îú‚îÄ‚îÄ upload_to_s3.py
‚îÇ   ‚îî‚îÄ‚îÄ load_to_snowflake.py
‚îú‚îÄ‚îÄ data/                      # Dados de exemplo
‚îÇ   ‚îî‚îÄ‚îÄ tweets_sample.json
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config_example.yaml    # Configura√ß√µes sens√≠veis (ex: S3, Snowflake)
‚îú‚îÄ‚îÄ docker-compose.yaml        # Ambientes Airflow
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env                       # Vari√°veis de ambiente (Airflow, AWS, Snowflake)
```

## üîÅ Fluxo do Pipeline

A DAG do Airflow executa as seguintes tarefas:

1. **Extrair dados do JSON**
2. **Transformar para .parquet**
3. **Fazer upload para AWS S3**
4. **Carregar no Snowflake**

## üì∏ Diagrama do Pipeline

> *(A imagem do fluxo ser√° adicionada aqui posteriormente)*

## ‚ñ∂Ô∏è Como Executar Localmente

### Pr√©-requisitos

- Docker + Docker Compose
- Conta na AWS com bucket S3 criado
- Conta na Snowflake com:
  - Warehouse: `COMPUTE_WH`
  - Database: `TWITTER_DATA_PIPELINE`
  - Schema: `RAW_DATA`
  - Tabela: `tweets` criada previamente

### 1. Clone o projeto

```bash
git clone https://github.com/dgcastrooo/twitter-data-papeline.git
cd twitter-data-papeline
```

### 2. Configure suas vari√°veis

Crie o arquivo `.env` com as seguintes vari√°veis (exemplo):

```env
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_BUCKET_NAME=your_bucket_name
AWS_REGION=sa-east-1

SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_DATABASE=TWITTER_DATA_PIPELINE
SNOWFLAKE_SCHEMA=RAW_DATA
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_TABLE=tweets
```

### 3. Suba os containers

```bash
docker-compose up -d
```

### 4. Acesse o Airflow

Abra no navegador:

```
http://localhost:8080
```

Login padr√£o:

- **Usu√°rio:** `airflow`
- **Senha:** `airflow`

## ‚úÖ Status das Tarefas

- [x] Extra√ß√£o de dados do JSON
- [x] Transforma√ß√£o para Parquet
- [x] Upload para AWS S3
- [x] Carga para Snowflake
- [x] Orquestra√ß√£o via Airflow

## üìÑ Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.

## üôã‚Äç‚ôÇÔ∏è Autor

**Diogo Jos√© Castro de Carvalho Pereira**  
[LinkedIn](https://www.linkedin.com/in/dgcastrooo/) ‚Ä¢ [GitHub](https://github.com/dgcastrooo)
